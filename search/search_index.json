{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Rockoon Controller documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Rockoon Controller is a Kubernetes operator that implements lifecycle management for OpenStack deployment.</p> <p>The Rockoon is written in Python using Kopf as a Python framework to build Kubernetes operators, and Pykube.</p> <p>The controller subscribes to changes to OpenStackDeployment Kubernetes custom resource and then reacts to these changes by creating, updating, or deleting appropriate resources in Kubernetes.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>File a bug: https://github.com/Mirantis/rockoon/issues</li> <li>Join slack channel</li> </ul>"},{"location":"#developer","title":"Developer","text":"<ul> <li>Contributing: https://TODO</li> <li>Reference Architecture:  https://mirantis.github.io/rockoon</li> </ul>"},{"location":"developer/","title":"Developer Guide","text":""},{"location":"developer/#code-style","title":"Code Style","text":"<p>Rockoon Contoller uses Black code formatter To check your chenages and format them use <pre><code>tox -e black\n</code></pre></p>"},{"location":"developer/#tests","title":"Tests","text":"<p>Each commit should require to pass code styles and unittests. To run unittests locally <pre><code>tox -e py310\n</code></pre></p>"},{"location":"developer/#running-controller-locally","title":"Running controller locally","text":"<p>Rockoon Controller is deployed as helm chart into kubernetes cluster. However there is possibility to run controller locally. For this: <pre><code>tox -e dev\n</code></pre></p>"},{"location":"architecture/rockoon-admission/","title":"OpenStack Controller Admission","text":"<p>The CustomResourceDefinition resource in Kubernetes uses the OpenAPI Specification version 2 to specify the schema of the resource defined. The Kubernetes API outright rejects the resources that do not pass this schema validation.</p> <p>The language of the schema, however, is not expressive enough to define a specific validation logic that may be needed for a given resource. For this purpose, Kubernetes enables the extension of its API with Dynamic Admission Control.</p> <p>For the OpenStackDeployment (OsDpl) CR the ValidatingAdmissionWebhook is a natural choice. It is deployed as part of OpenStack Controller in dedicated deployment by default and performs specific extended validations when an <code>OpenStackDeployment</code> CR is created or updated.</p> <p>The inexhaustive list of additional validations includes:</p> <ul> <li>Deny the OpenStack version downgrade</li> <li>Deny the OpenStack version skip-level upgrade</li> <li>Deny the OpenStack master version deployment</li> <li>Deny upgrade to the OpenStack master version</li> <li>Deny deploying invalid configuration</li> </ul>"},{"location":"architecture/cloud_services/horizon/","title":"Horizon Configuration","text":"<p>This article describes horizon configuration.</p>"},{"location":"architecture/cloud_services/horizon/#custom-themes","title":"Custom Themes","text":"<p>To apply custom horizon theme on your environment use the following snippet for <code>OpenStackDeployment</code> custom resource:</p> <pre><code>spec:\n  features:\n    horizon:\n      themes:\n      - description: my custom theme\n        name: AwesomTheme\n        url: https://url/to/theme.tar.gz\n        sha256summ: \"sha256 of theme archive\"\n        enabled: true\n</code></pre> <p>By default <code>Mirantis</code> theme is enabled, to disabled it use</p> <pre><code>spec:\n  features:\n    horizon:\n      themes:\n      - name: mirantis\n        enabled: false\n</code></pre>"},{"location":"architecture/cloud_services/masakari/","title":"Instance High Availability service (OpenStack Masakari)","text":"<p>This article describes operations with Instance High Availability (Instance HA).</p> <p>The Instance High Availability service (OpenStack Masakari) allows cloud users to ensure that their instances are automatically evacuated from a failed hypervisor. It provides several types of monitoring services:</p> <ul> <li><code>Instance monitor</code> \u2014 checks the liveness of instance processes.</li> <li><code>Introspective instance monitor</code> \u2014 improves instance high availability within   OpenStack environments by monitoring and identifying system-level failures   through the QEMU Guest Agent.</li> <li><code>Host monitor</code> \u2014 checks the liveness of compute hosts and runs as part of the   Node Controller in Rockoon.</li> </ul> <p>The <code>introspective instance monitor</code> is disabled by default and must be explicitly enabled in the Masakari configuration.</p>"},{"location":"architecture/cloud_services/masakari/#enabling-the-instance-ha-service","title":"Enabling the Instance HA service","text":"<p>To enable the Instance HA service your need to add <code>instance-ha</code> to the service list in OpenStackDeployment custom resource:</p> <pre><code>spec:\n  features:\n    services:\n      - instance-ha\n</code></pre>"},{"location":"architecture/cloud_services/masakari/#enabling-introspective-instance-monitor","title":"Enabling introspective instance monitor","text":"<p>To enable the introspective instance monitor in the Masakari service, update the <code>spec:features:masakari:monitors:introspective</code> section in OpenStackDeployment custom resource:</p> <pre><code>spec:\n  features:\n    masakari:\n      monitors:\n        introspective:\n          enabled: true\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeployment/","title":"OpenStackDeployment Custom Resource","text":"<p>Custom Kubernetes resource that describes OpenStack deployment.</p> <pre><code>kubectl get crd openstackdeployments.lcm.mirantis.com -o yaml\n</code></pre> <pre><code>kubectl -n openstack get osdpl -o yaml\n</code></pre> <p>Example of OpenStackDeployment minimal configuration</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: OpenStackDeployment\nmetadata:\n  annotations:\n  name: osh-dev\n  namespace: openstack\nspec:\n  features:\n    glance:\n      backends:\n        file:\n          pvcstore:\n            default: true\n            pvc:\n              size: 10Gi\n              storage_class_name: lvp-fake-root\n    network_policies:\n      enabled: false\n    neutron:\n      external_networks:\n      - bridge: br-ex\n        interface: veth-phy\n        network_types:\n        - flat\n        physnet: physnet1\n      floating_network:\n        enabled: true\n        physnet: physnet1\n        subnet:\n          gateway: 10.11.12.11\n          pool_end: 10.11.12.200\n          pool_start: 10.11.12.100\n          range: 10.11.12.0/24\n      tunnel_interface: ens3\n    nova:\n      images:\n        backend: local\n      live_migration_interface: ens3\n    services: []\n    ssl:\n      public_endpoints:\n        api_cert:\n          value_from:\n            secret_key_ref:\n              key: api_cert\n              name: osh-dev-hidden\n        api_key:\n          value_from:\n            secret_key_ref:\n              key: api_key\n              name: osh-dev-hidden\n        ca_cert:\n          value_from:\n            secret_key_ref:\n              key: ca_cert\n              name: osh-dev-hidden\n  local_volume_storage_class: lvp-fake-root\n  openstack_version: caracal\n  persistent_volume_storage_class: lvp-fake-root\n  preset: core\n  public_domain_name: it.just.works\n  size: single\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeployment/#main-osdpl-elements","title":"Main osdpl elements","text":"<p>Main elements of OpenStackDeployment custom resource</p> <ul> <li><code>spec.openstack_version</code>: Specifies the OpenStack release to deploy</li> <li><code>spec.preset</code>: String that specifies the name of the preset, a predefined configuration for the OpenStack cluster. A preset includes:<ul> <li>A set of enabled services that includes virtualization, bare metal management, secret management, and others</li> <li>Major features provided by the services, such as VXLAN encapsulation of the tenant traffic</li> </ul> </li> <li><code>spec.size</code>: String that specifies the size category for the OpenStack cluster. The size category defines the internal configuration   of the cluster such as the number of replicas for service workers and timeouts, etc.   The list of supported sizes include:<ul> <li><code>single</code>: single node installation</li> <li><code>tiny</code>: for approximately 10 OpenStack Compute nodes</li> <li><code>small</code>:  for approximately 50 OpenStack Compute nodes</li> <li><code>medium</code>: for approximately 300+ OpenStack Compute nodes  </li> </ul> </li> <li><code>spec.public_domain_name</code>: Specifies the public DNS name for OpenStack services. This is a base DNS name that must be accessible and   resolvable by API clients of your OpenStack cloud. It will be present in the OpenStack endpoints as presented by the OpenStack Identity   service catalog. The TLS certificates used by the OpenStack services (see below) must also be issued to this DNS name. </li> <li><code>spec.features</code>: Contains the top-level collections of settings for the OpenStack deployment that potentially target several OpenStack services.   The section where the customizations should take place.</li> </ul>"},{"location":"architecture/custom-resources/openstackdeployment/#handling-sensitive-information","title":"Handling sensitive information","text":"<p>The <code>OpenStackDeployment</code> custom resource enables you to securely store sensitive fields in Kubernetes secrets. To do that, verify that the reference secret is present in the same namespace as the <code>OpenStackDeployment</code> object and the <code>openstack.lcm.mirantis.com/osdpl_secret</code> label is set to <code>true</code>. The list of fields that can be hidden from <code>OpenStackDeployment</code> is limited and defined by the <code>OpenStackDeployment</code> schema.</p> <p>For example, to hide spec:features:ssl:public_endpoints:api_cert, use the following structure:</p> <pre><code>spec:\n  features:\n    ssl:\n      public_endpoints:\n        api_cert:\n          value_from:\n            secret_key_ref:\n              key: api_cert\n              name: osh-dev-hidden\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/","title":"OpenStackDeploymentStatus Custom Resource","text":"<p>The resource of kind <code>OpenStackDeploymentStatus</code> is a custom resource that describes the status of an OpenStack deployment. To obtain detailed information about the schema of an OpenStackDeploymentStatus custom resource:</p> <pre><code>kubectl get crd openstackdeploymentstatus.lcm.mirantis.com -o yaml\n</code></pre> <p>To obtain the status definition for a particular OpenStack deployment: <pre><code>kubectl -n openstack get osdplst\n</code></pre></p> <p>Example of system response:</p> <pre><code>NAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   antelope            0.16.1.dev104        APPLIED   20/20          21/22    MOSK 24.1.3\n</code></pre> <p>Where:</p> <ul> <li><code>OPENSTACK VERSION</code> displays the actual OpenStack version of the deployment</li> <li><code>CONTROLLER VERSION</code> indicates the version of the Rockoon controller responsible for the deployment</li> <li><code>STATE</code> reflects the current status of life-cycle management. The list of possible values includes:</li> <li><code>APPLYING</code> indicates that some Kubernetes objects for applications are in the process of being applied</li> <li><code>APPLIED</code> indicates that all Kubernetes objects for applications have been applied to the latest state</li> <li><code>LCM PROGRESS</code> reflects the current progress of STATE in the format X/Y, where X denotes the number of applications with Kubernetes objects applied and in the actual state, and Y represents the total number of applications managed by the Rockoon controller</li> <li><code>HEALTH</code> provides an overview of the current health status of the OpenStack deployment in the format X/Y, where X represents the number of applications with notReady pods, and Y is the total number of applications managed by the Rockoon controller</li> <li><code>MOSK RELEASE</code> displays the current product release of the OpenStack deployment</li> </ul> <p>Example of <code>OpenStackDeploymentStatus</code></p> <pre><code>kind: OpenStackDeploymentStatus\nmetadata:\n  name: osh-dev\n  namespace: openstack\nspec: {}\nstatus:\n  handle:\n    lastStatus: update\n  health:\n    barbican:\n      api:\n        generation: 2\n        status: Ready\n    cinder:\n      api:\n        generation: 2\n        status: Ready\n      backup:\n        generation: 1\n        status: Ready\n      scheduler:\n        generation: 1\n        status: Ready\n      volume:\n        generation: 1\n        status: Ready\n  osdpl:\n    cause: update\n    changes: '((''add'', (''status'',), None, {''watched'': {''ceph'': {''secret'':\n      {''hash'': ''0fc01c5e2593bc6569562b451b28e300517ec670809f72016ff29b8cbaf3e729''}}}}),)'\n    controller_version: 0.5.3.dev12\n    fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n    openstack_version: ussuri\n    state: APPLIED\n    timestamp: \"2021-09-08 17:01:45.633143\"\n  services:\n    baremetal:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:54.081353\"\n    block-storage:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:57.306669\"\n    compute:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:18.853068\"\n    coordination:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.593719\"\n    dashboard:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:57.652145\"\n    database:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.233777\"\n    dns:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:56.540886\"\n    identity:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:00.961175\"\n    image:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.976976\"\n    ingress:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:01.440757\"\n    key-manager:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:51.822997\"\n    load-balancer:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:02.462824\"\n    memcached:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:03.165045\"\n    messaging:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.637506\"\n    networking:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:35.553483\"\n    object-storage:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:01.828834\"\n    orchestration:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:01:02.846671\"\n    placement:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:58.039210\"\n    redis:\n      controller_version: 0.5.3.dev12\n      fingerprint: a112a4a7d00c0b5b79e69a2c78c3b50b0caca76a15fe7d79a6ad1305b19ee5ec\n      openstack_version: ussuri\n      state: APPLIED\n      timestamp: \"2021-09-08 17:00:36.562673\"\n</code></pre>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#health-structure","title":"Health structure","text":"<p>The <code>health</code> subsection provides a brief output on services health of each component</p>"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#osdpl-structure","title":"OsDpl structure","text":"<p>The <code>osdpl</code> subsection describes the overall status of the OpenStack deployment.</p> Element Description <code>cause</code> The cause that triggered the LCM action: <code>update</code> when OsDpl is updated, <code>resume</code> when the OpenStack Controller is restarted <code>changes</code> A string representation of changes in the <code>OpenstackDeployment</code> object <code>controller_version</code> The version of <code>rockoon</code> that handles the LCM action <code>fingerprint</code> The SHA sum of the <code>OpenStackDeployment</code> object spec section <code>openstack_version</code> The current OpenStack version specified in the <code>osdpl</code> object <code>state</code> The current state of the LCM action. - <code>APPLYING</code>: not all operations are completed  - <code>APPLIED</code>: all operations are completed <code>timestamp</code> The timestamp of the status:osdpl section update"},{"location":"architecture/custom-resources/openstackdeploymentstatus/#services-structure","title":"Services structure","text":"<p>The services subsection provides detailed information of LCM performed with a specific service. This is a dictionary where keys are service names, for example, <code>baremetal</code> or <code>compute</code> and values are dictionaries with the following items.</p> <p>Services structure elements</p> Element Description <code>controller_version</code> The version of the <code>rockoon</code> that handles the LCM action on a specific service <code>fingerprint</code> The SHA sum of the <code>OpenStackDeployment</code> object spec section used when performing the LCM on a specific service <code>openstack_version</code> The OpenStack version specified in the <code>osdpl</code> object used when performing the LCM action on a specific service <code>state</code> The current state of the LCM action. - <code>WAITING</code>: waiting for dependencies  - <code>APPLYING</code>: not all operations are completed  - <code>APPLIED</code>: all operations are completed <code>timestamp</code> The timestamp of the status:osdpl section update"},{"location":"architecture/rockoon/configuration/","title":"Configuration","text":"<p>The OpenStack Controller enables you to modify its configuration at runtime without restarting. MOSK stores the controller configuration in the <code>openstack-controller-config</code> <code>ConfigMap</code> in the osh-system namespace of your cluster.</p> <p>To retrieve the OpenStack Controller configuration <code>ConfigMap</code>, run:</p> <pre><code>kubectl get configmaps openstack-controller-config -o yaml\n</code></pre> <p>Example of OpenStackController configuration</p> <pre><code>apiVersion: v1\ndata:\n  extra_conf.ini: |\n    [maintenance]\n    respect_nova_az = false\nkind: ConfigMap\nmetadata:\n  annotations:\n    openstackdeployments.lcm.mirantis.com/skip_update: \"true\"\n  name: openstack-controller-config\n  namespace: osh-system\n</code></pre> <pre><code>[osctl]\n# The number of seconds to wait for all component from application becomes ready\nwait_application_ready_timeout = 1200\n\n# The number of seconds to sleep between checking application ready attempts\nwait_application_ready_delay = 10\n\n# The amount of time to wit for flapping node\nnode_not_ready_flapping_timeout = 120\n\n[helmbundle]\n# The number of seconds to wait for values set in manifest are propagated to child objects.\nmanifest_enable_timeout = 600\n\n# The number of seconds between attempts to check that values were applied.\nmanifest_enable_delay = 10\n\n# The number of seconds to wait for values are removed from manifest and propagated to child objects.\nmanifest_disable_timeout = 600\n\n# The number of seconds between attempts to check that values were removed from release.\nmanifest_disable_delay = 10\n\n# The number of seconds to wait for kubernetes object removal\nmanifest_purge_timeout = 600\n\n# The number of seconds between attempts to check that kubernetes object is removed\nmanifest_purge_delay = 10\n\n# The number of seconds to pause for helmbundle changes\nmanifest_apply_delay = 10\n\n# The number of seconds to run for helm command\nhelm_cmd_timeout = 120\n\n[maintenance]\n# number of instances to migrate concurrently\ninstance_migrate_concurrency = 1\n\n# max number of compute nodes we allow to update in parallel\nnwl_parallel_max_compute = 30\n\n# max number of gateway nodes we allow to update in parallel\nnwl_parallel_max_gateway = 1\n\n# respect nova AZs, when set to true parallel update is allowed only for computes in same AZ\nrespect_nova_az = True\n\n# flag to skip instance check on host before proceeding with node removal. By default is False\n# which means that node removal will be blocked unless at least 1 instance exists on host.\nndr_skip_instance_check = False\n\n# flag to skip volume check on host before proceeding with node removal. By default is False\n# which means that node removal will be blocked unless at least 1 volume exists on host.\n# Volume is tied to specific host only for LVM backend.\nndr_skip_volume_check = False\n</code></pre>"},{"location":"architecture/rockoon/custom-images/","title":"Customize images","text":"<p>OpenStack Controller has default built in images that were verified against different production configurations. However it may be needed to inclide additional patches into openstack code or 3rd party software.</p> <p>OpenStack images are built with help of Loci. Please refer to its documentation to get more detail about build process.</p> <p>To inject a custom image create configmap with <code>&lt;openstackdeployment-name&gt;-artifacts</code> name in <code>openstack</code> namespace and folling data structure:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: osh-dev-artifacts\n  namespace: openstack\ndata:\n  caracal: |\n    libvirt: docker-dev-kaas-virtual.mcp.mirantis.com/general/libvirt:6.0.0-focal-20221028120749\n  antelope: |\n    libvirt: docker-dev-kaas-virtual.mcp.mirantis.com/general/libvirt:6.0.0-focal-20221028120749\n</code></pre>"},{"location":"architecture/rockoon/overview/","title":"OpenStack Controller","text":"<p><code>OpenStack controller</code> is running as a deployment in Kubernetes with multiple subcontrollers that are running as dedicated containers in the deployment. Each subcontroller </p> Container Description <code>osdpl</code> The core subcontroller that handles changes of <code>OpenStackDeployment</code> object <code>secrets</code> Subcontroller that provides data excange between different components <code>health</code> Subcontroller that constantly watching for OpenStack health and reporting its status <code>node</code> Subcontroller that watches for <code>Node</code> object <code>nodemaintenancerequest</code> Subcontroller that provides integration with Kubernetes lifecycle management <code>ceph-secrets</code> Subcontroller that provides integration with <code>Ceph</code> storage <code>osdplstatus</code> Subcontroller responsible for status reporting <code>tf-secrets</code> Subcontroller that provides integration with TungstenFabric"},{"location":"ops/openstack/getting-access/","title":"Access OpenStack","text":"<p>This section explains how to access your OpenStack environment as admin user. Before you proceed, make sure that you can access the Kubernetes API.</p>"},{"location":"ops/openstack/getting-access/#built-in-admin-cli","title":"Built-in admin CLI","text":"<p>You can use the built-in admin CLI client and execute the openstack commands from a dedicated pod deployed in the <code>openstack</code> namespace:</p> <pre><code>kubectl -n openstack exec -it deployment/keystone-client -- bash\n</code></pre> <p>This pod has <code>python-openstackclient</code> and all required plugins already installed. The <code>python-openstackclient</code> command-line client is configured to use the admin user credentials. To view the detailed configuration for the OpenStack run the command in the pod:</p> <pre><code>cat /etc/openstack/clouds.yaml\n</code></pre>"},{"location":"ops/openstack/getting-access/#horizon","title":"Horizon","text":"<p>Before you proceed, make sure that you can access to Horizon URL from your workplace. In case of AIO installation you can use VPN according to this article.</p> <ol> <li>Obtain the admin user credentials from the <code>openstack-identity-credentials</code> secret in the <code>openstack-external</code> namespace: <pre><code>kubectl -n openstack-external get secrets openstack-identity-credentials -o jsonpath='{.data.clouds\\.yaml}' | base64 -d\n</code></pre> Example of a system response: <pre><code>clouds:\n  admin:\n    auth:\n      auth_url: https://keystone.it.just.works/\n      password: &lt;ADMIN_PWD&gt;\n      project_domain_name: &lt;ADMIN_PROJECT_DOMAIN&gt;\n      project_name: &lt;ADMIN_PROJECT&gt;\n      user_domain_name: &lt;ADMIN_USER_DOMAIN&gt;\n      username: &lt;ADMIN_USER_NAME&gt;\n    endpoint_type: public\n    identity_api_version: 3\n    interface: public\n    region_name: CustomRegion\n  admin-system:\n    auth:\n      auth_url: https://keystone.it.just.works/\n      password: &lt;ADMIN_PWD&gt;\n      system_scope: all\n      user_domain_name: &lt;ADMIN_USER_DOMAIN&gt;\n      username: &lt;ADMIN_USER_NAME&gt;\n    endpoint_type: public\n    identity_api_version: 3\n    interface: public\n    region_name: CustomRegion\n</code></pre></li> <li>Access Horizon through your browser using its public service. By default for Rockoon AIO installation it's https://horizon.it.just.works. To log in, specify the user name and domain name obtained in previous step from the <code>&lt;ADMIN_USER_NAME&gt;</code>, <code>&lt;ADMIN_PWD&gt;</code> and <code>&lt;ADMIN_USER_DOMAIN&gt;</code> values.</li> </ol> <p>If OpenStack was deployed with self-signed TLS certificates for public endpoints, you may get a warning about an untrusted certificate. To proceed, allow the connection.</p>"},{"location":"ops/openstack/getting-access/#cli-from-your-local-machine","title":"CLI from your local machine","text":"<p>To be able to access your OpenStack environment through the CLI, you need to configure the openstack client environment using either an <code>openstackrc</code> environment file or <code>clouds.yaml</code> file.</p> <ul> <li>openstackrc<ol> <li>Log in to Horizon as described in previous chapter</li> <li>Download the <code>openstackrc</code> file from the web UI.</li> <li>On any shell from which you want to run OpenStack commands, source the environment    file for the respective project.</li> </ol> </li> <li> <p>clouds.yaml</p> <ol> <li> <p>Obtain clouds.yaml: <pre><code>mkdir -p ~/.config/openstack\nkubectl -n openstack-external get secrets openstack-identity-credentials -o jsonpath='{.data.clouds\\.yaml}' | base64 -d &gt; ~/.config/openstack/clouds.yaml\n</code></pre>   The OpenStack client looks for <code>clouds.yaml</code> in the following locations:</p> <ul> <li>current directory</li> <li>~/.config/openstack</li> <li>/etc/openstack.</li> </ul> </li> <li> <p>Export the OS_CLOUD environment variable: <pre><code>export OS_CLOUD=admin\n</code></pre></p> </li> </ol> </li> </ul> <p>Now, you can use the openstack CLI as usual. For example: <pre><code>openstack user list\n</code></pre> Example of an expected system response: <pre><code>+----------------------------------+-----------------+\n| ID                               | Name            |\n+----------------------------------+-----------------+\n| dc23d2d5ee3a4b8fae322e1299f7b3e6 | internal_cinder |\n| 8d11133d6ef54349bd014681e2b56c7b | admin           |\n+----------------------------------+-----------------+\n</code></pre></p> <p>If OpenStack was deployed with self-signed TLS certificates for public endpoints, you may need to use the openstack command-line client with certificate validation disabled. For example: <pre><code>openstack --insecure user list\n</code></pre></p>"},{"location":"ops/openstack/tempest/","title":"Run tempest tests","text":"<p>The OpenStack Integration Test Suite (Tempest), is a set of integration tests to be run against a live OpenStack environment. This section instructs you on how to verify the workability of your OpenStack deployment using Tempest.</p> <p>To verify an OpenStack deployment using Tempest:</p> <ol> <li>Add <code>tempest</code> to <code>spec:features:services</code> in <code>OpenStackDeployment</code> custom resource.</li> <li>Wait until Tempest is ready. The Tempest tests are launched by the <code>openstack-tempest-run-tests</code> job. To    keep track of the tests execution, run:    <pre><code>kubectl -n openstack logs -l application=tempest,component=run-tests\n</code></pre></li> <li>Get the Tempest results. The Tempest results can be stored in a <code>pvc-tempest</code> PersistentVolumeClaim (PVC).    To get them from a PVC, use:    <pre><code># Run pod and mount pvc to it\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: tempest-test-results-pod\n  namespace: openstack\nspec:\n  nodeSelector:\n    openstack-control-plane: enabled\n  volumes:\n    - name: tempest-pvc-storage\n      persistentVolumeClaim:\n        claimName: pvc-tempest\n  containers:\n    - name: tempest-pvc-container\n      image: ubuntu\n      command: ['sh', '-c', 'sleep infinity']\n      volumeMounts:\n        - mountPath: \"/var/lib/tempest/data\"\n          name: tempest-pvc-storage\nEOF\n</code></pre></li> </ol> <p>To rerun tempest:</p> <ol> <li>Remove <code>tempest</code> from the list of enabled services.</li> <li>Wait until Tempest jobs are removed.</li> <li>Add <code>tempest</code> back to the list of the enabled services.</li> </ol>"},{"location":"ops/openstack/troubleshoot/","title":"Troubleshooting","text":"<p>This section provides the general debugging instructions for your OpenStack on Kubernetes deployment. Start your troubleshooting with the determination of the failing component that can include the Rockoon Operator, Helm, a particular pod or service.</p> <p>Note</p> <p>For Kubernetes cluster debugging and troubleshooting, refer to Kubernetes official documentation: Troubleshoot clusters</p>"},{"location":"ops/openstack/troubleshoot/#debugging-the-helm-releases","title":"Debugging the Helm releases","text":"<ol> <li>Log in to the <code>rockoon</code> pod, where the Helm v3 client is installed, or download the Helm v3 binary locally: <pre><code>kubectl -n osh-system exec -it deployment/rockoon -- bash\n</code></pre></li> <li>Verify the Helm releases statuses: <pre><code>helm3 --namespace openstack list --all\n</code></pre> Example of output: <pre><code>NAME                            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\netcd                            openstack       4               2021-07-09 11:06:25.377538008 +0000 UTC deployed        etcd-0.1.0-mcp-2735\ningress-openstack               openstack       4               2021-07-09 11:06:24.892822083 +0000 UTC deployed        ingress-0.1.0-mcp-2735\nopenstack-barbican              openstack       4               2021-07-09 11:06:25.733684392 +0000 UTC deployed        barbican-0.1.0-mcp-3890\nopenstack-ceph-rgw              openstack       4               2021-07-09 11:06:25.045759981 +0000 UTC deployed        ceph-rgw-0.1.0-mcp-2735\nopenstack-cinder                openstack       4               2021-07-09 11:06:42.702963544 +0000 UTC deployed        cinder-0.1.0-mcp-3890\nopenstack-designate             openstack       4               2021-07-09 11:06:24.400555027 +0000 UTC deployed        designate-0.1.0-mcp-3890\nopenstack-glance                openstack       4               2021-07-09 11:06:25.5916904   +0000 UTC deployed        glance-0.1.0-mcp-3890\nopenstack-heat                  openstack       4               2021-07-09 11:06:25.3998706   +0000 UTC deployed        heat-0.1.0-mcp-3890\nopenstack-horizon               openstack       4               2021-07-09 11:06:23.27538297  +0000 UTC deployed        horizon-0.1.0-mcp-3890\nopenstack-iscsi                 openstack       4               2021-07-09 11:06:37.891858343 +0000 UTC deployed        iscsi-0.1.0-mcp-2735            v1.0.0\nopenstack-keystone              openstack       4               2021-07-09 11:06:24.878052272 +0000 UTC deployed        keystone-0.1.0-mcp-3890\nopenstack-libvirt               openstack       4               2021-07-09 11:06:38.185312907 +0000 UTC deployed        libvirt-0.1.0-mcp-2735\nopenstack-mariadb               openstack       4               2021-07-09 11:06:24.912817378 +0000 UTC deployed        mariadb-0.1.0-mcp-2735\nopenstack-memcached             openstack       4               2021-07-09 11:06:24.852840635 +0000 UTC deployed        memcached-0.1.0-mcp-2735\nopenstack-neutron               openstack       4               2021-07-09 11:06:58.96398517  +0000 UTC deployed        neutron-0.1.0-mcp-3890\nopenstack-neutron-rabbitmq      openstack       4               2021-07-09 11:06:51.454918432 +0000 UTC deployed        rabbitmq-0.1.0-mcp-2735\nopenstack-nova                  openstack       4               2021-07-09 11:06:44.277976646 +0000 UTC deployed        nova-0.1.0-mcp-3890\nopenstack-octavia               openstack       4               2021-07-09 11:06:24.775069513 +0000 UTC deployed        octavia-0.1.0-mcp-3890\nopenstack-openvswitch           openstack       4               2021-07-09 11:06:55.271711021 +0000 UTC deployed        openvswitch-0.1.0-mcp-2735\nopenstack-placement             openstack       4               2021-07-09 11:06:21.954550107 +0000 UTC deployed        placement-0.1.0-mcp-3890\nopenstack-rabbitmq              openstack       4               2021-07-09 11:06:25.431404853 +0000 UTC deployed        rabbitmq-0.1.0-mcp-2735\nopenstack-tempest               openstack       2               2021-07-09 11:06:21.330801212 +0000 UTC deployed        tempest-0.1.0-mcp-3890\n</code></pre></li> </ol>"},{"location":"ops/openstack/troubleshoot/#debugging-the-rockoon-controller","title":"Debugging the Rockoon Controller","text":"<p>The Rockoon Controller is running in several containers in the <code>rockoon-xxxx</code> pod in the <code>osh-system</code> namespace.</p> <p>To verify the status of the Rockoon Controller, run: <pre><code>kubectl -n osh-system get pods\n</code></pre></p> <p>Example of a system response: <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\nrockoon-5c6947c996-vlrmv            5/5     Running     0          17m\nrockoon-admission-f946dc8d6-6bgn2   1/1     Running     0          4h9m\nrockoon-ensure-resources-5ls8k        0/1     Completed   0          4h12m\n</code></pre></p> <p>To verify the logs for the <code>osdpl</code> container, run: <pre><code>kubectl -n osh-system logs -f &lt;rockoon-xxxx&gt; -c osdpl\n</code></pre></p>"},{"location":"ops/openstack/troubleshoot/#some-pods-are-stuck-in-init","title":"Some pods are stuck in <code>Init</code>","text":"<p>MOSK uses the Kubernetes entrypoint init container to resolve dependencies between objects. If the pod is stuck in Init:0/X, this pod may be waiting for its dependencies.</p> <p>Verify the missing dependencies: <pre><code>kubectl -n openstack logs -f placement-api-84669d79b5-49drw -c init\n</code></pre></p> <p>Example of a system response: <pre><code>Entrypoint WARNING: 2020/04/21 11:52:50 entrypoint.go:72: Resolving dependency Job placement-ks-user in namespace openstack failed: Job Job placement-ks-user in namespace openstack is not completed yet .\nEntrypoint WARNING: 2020/04/21 11:52:52 entrypoint.go:72: Resolving dependency Job placement-ks-endpoints in namespace openstack failed: Job Job placement-ks-endpoints in namespace openstack is not completed yet .\n</code></pre></p>"},{"location":"ops/openstack/upgrade/","title":"Upgrade OpenStack","text":"<p>This section provides instructions on how to upgrade OpenStack to a major version with help of OpenStack Controller.</p> <ol> <li> <p>To start the OpenStack upgrade, change the value of the <code>spec:openstack_version</code> parameter in the <code>OpenStackDeployment</code> object to the target OpenStack release.    After you change the value of the <code>spec:openstack_version</code> parameter, the OpenStack Controller initializes the upgrade process.</p> </li> <li> <p>Verify the upgrade status    <pre><code>kubectl -n openstack get osdplst\n</code></pre>    Example of output    <pre><code>NAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   antelope             0.17.2.dev250        APPLYING   1/11          13/15 \n</code></pre>    When upgrade finishes, the <code>STATE</code> field should display <code>APPLIED</code>:    <pre><code>kubectl -n openstack get osdplst\nNAME      OPENSTACK VERSION   CONTROLLER VERSION   STATE     LCM PROGRESS   HEALTH   MOSK RELEASE\nosh-dev   caracal             0.17.2.dev250        APPLIED   11/11          15/15\n</code></pre></p> </li> <li> <p>Verify the Upgrade</p> </li> <li>Verify that OpenStack is healthy and operational. All OpenStack components in the <code>health</code> group in the      OpenStackDeploymentStatus CR should be in the <code>Ready</code> state.</li> <li>Verify the workability of your OpenStack deployment by running Tempest against the OpenStack cluster as described in Run Tempest tests.</li> </ol>"},{"location":"quick-start/aio-installation-manual/","title":"All in One Installation","text":"<p>This paragraph provides a guide how to deploy single node deployment with k0s based Kubernetes cluster and openstack deployed by Rockoon controller.</p>"},{"location":"quick-start/aio-installation-manual/#example-command-to-create-an-appropriate-kvm-vm-on-linux-with-virt-manager","title":"Example command to create an appropriate kvm VM on Linux with <code>virt-manager</code>","text":"<p>Needs <code>virt-manager</code>, <code>libvirt</code>, and <code>qemu-kvm</code> installed.</p> <p>Change the path to the public SSH key as you need, the user in the image is <code>ubuntu</code>.</p> <p><pre><code>wget https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\nqemu-img create -F qcow2 -f qcow2 -b noble-server-cloudimg-amd64.img rockoon.qcow2 100G\nvirt-install \\\n    --name rockoon \\\n    --import \\\n    --disk path=$PWD/rockoon.qcow2,format=qcow2 \\\n    --vcpus=8 \\\n    --memory=16384 \\\n    --network \"network=default,model=virtio\" \\\n    --osinfo \"ubuntu24.04\" \\\n    --arch x86_64 \\\n    --graphics vnc,listen=0.0.0.0 \\\n    --cloud-init clouduser-ssh-key=$HOME/.ssh/id_rsa.pub \\\n    --virt-type kvm \\\n    --watchdog=default \\\n    --noautoconsole\n</code></pre> Once the VM is running, run <code>virsh domifaddr rockoon</code> to find the IP address of the VM to SSH to.</p>"},{"location":"quick-start/aio-installation-manual/#example-command-to-create-an-appropriate-qemu-vm-on-macos-with-lima","title":"Example command to create an appropriate qemu VM on MacOS with <code>lima</code>","text":"<p>As mentioned, only Intel-based Macs are currently supported.</p> <pre><code>brew install lima\nlimactl create \\\n    --name=rockoon \\\n    --tty=false \\\n    --cpus 8 \\\n    --disk 100 \\\n    --memory 16 \\\n    --plain \\\n    --arch x86_64 \\\n    --vm-type qemu \\\n    --set '.cpuType.x86_64 = \"host\"' \\\n    template://ubuntu-24.04\nlimactl start rockoon\nssh -F ~/.lima/rockoon/ssh.config lima-rockoon\n</code></pre>"},{"location":"quick-start/aio-installation-manual/#trigger-deployment","title":"Trigger Deployment","text":"<ol> <li> <p>Download repository with rockoon   <pre><code>git clone https://github.com/Mirantis/rockoon\n</code></pre></p> </li> <li> <p>Trigger deployment   <pre><code>cd rockoon/virtual_lab/\nsudo bash install.sh\n</code></pre></p> </li> </ol>"},{"location":"quick-start/introduction/","title":"Introduction","text":"<p>Rockoon can be deployed in different ways. For a quick start in a preconfigured environment, we have created the TryMOSK image, which allows you to easily launch a virtual machine with Rockoon and additional tools. The process of deploying Rockoon using the TryMOSK image is described in the TryMOSK (Using pre-built image) section.</p> <p>Another option is to deploy Rockoon on your own existing virtual machine. In this case, the sources will be downloaded from GitHub, and some of the Docker images will be built on the virtual machine during the deployment process. This method is described in the Manual Install (Advanced) section.</p>"},{"location":"quick-start/introduction/#host","title":"Host","text":"<p>At the moment hosts with non-x86_64 CPU (like Apple Silicon) are not supported. The required full CPU emulation for virtual machine introduces too much overhead, making the system too slow and unusable.</p>"},{"location":"quick-start/introduction/#prepare-vm","title":"Prepare VM","text":"<p>For the deployment we will need Virtual Machine with following minimal requirements.</p> <p>Minimal VM requirements</p> Resource Amount RAM 16Gb CPU 4 DISK 40Gb <p>Supported operation systems for Manual install (Advanced)</p> <ul> <li>Ubuntu 24.04 (x86_64)</li> </ul>"},{"location":"quick-start/local-clients/","title":"Using kubectl and helm with Rockoon AIO","text":"<p>To configure and manage Rockoon AIO, you need the kubectl and helm command-line tools. These tools are already installed on your Rockoon AIO instance, but you can also install them locally on your computer.</p> <ol> <li> <p>Install kubectl and helm locally. Follow the official installation instructions:</p> <ul> <li>kubectl: https://kubernetes.io/docs/tasks/tools/#kubectl</li> <li>helm: https://helm.sh/docs/intro/install/</li> </ul> </li> <li> <p>Retrieve the configuration file from the Rockoon AIO instance. The kubeconfig file for Rockoon AIO is located on the instance at /root/.kube/config. You can retrieve it over SSH. For Linux/macOS, run: <pre><code>ssh -l ubuntu 18.218.29.107 sudo cat /root/.kube/config &gt; ~/aio_kube_config.yaml\n</code></pre> In this example:</p> <ul> <li><code>18.218.29.107</code> is the public IPv4 address of your Rockoon AIO instance.</li> <li>The file is saved locally as <code>aio_kube_config.yaml</code> in your home directory.</li> </ul> </li> <li> <p>Use kubectl and helm with the configuration file When connected to the Rockoon AIO instance via sshuttle or OpenVPN, you can run commands like: <pre><code>kubectl --kubeconfig ~/aio_kube_config.yaml ...\nhelm --kubeconfig ~/aio_kube_config.yaml ...\n</code></pre></p> <p>Note: If you want to avoid specifying <code>--kubeconfig</code> each time, move the configuration file to ~/.kube/config. Warning: This will overwrite existing Kubernetes configuration file at that location.</p> </li> <li> <p>Handling self-signed SSL certificates for TryMOSK installation TryMOSK uses a self-signed SSL certificate for its proxy-server. If running <code>helm</code> commands from your local computer, you must add <code>--insecure-skip-tls-verify</code> flag to skip TLS verification.</p> </li> </ol>"},{"location":"quick-start/trymosk-installation-aws/","title":"Installation on AWS","text":"<p>You can get a TryMOSK image from the Mirantis CDN server The latest image releases are located in folders with pefix <code>mosk-</code> in the name. You can also use a prepared AMI image on Amazon servers. This article describes the steps required to deploy TryMOSK from a prepared AMI image. At present time they are available on the <code>us-east-2</code> region.</p>"},{"location":"quick-start/trymosk-installation-aws/#creating-a-trymosk-instance-using-the-amazon-ec2-console","title":"Creating a TryMOSK Instance Using the Amazon EC2 Console","text":"<p>Step 1: Open the Amazon EC2 Console</p> <ol> <li>Sign in to the AWS Management Console.</li> <li>Open the Amazon EC2 console at: https://console.aws.amazon.com/ec2/.</li> </ol> <p>Step 2: Select the AWS Region</p> <ol> <li>In the navigation bar at the top-right, locate the Region selector.</li> <li>Select US East (Ohio).    </li> </ol> <p>Step 3: Create a Security Group</p> <ol> <li>In the left navigation pane, under Network &amp; Security, choose Security Groups.</li> <li>Click Create security group.</li> <li>In the Basic details section, provide:</li> <li>Security group name: A descriptive name (for example: <code>trymosk-sg</code>).</li> <li>Description: Purpose of the security group.</li> <li>In the Inbound rules section:</li> <li>Click Add rule.</li> <li>For Type, choose SSH.</li> <li>If you plan to use OpenVPN, click Add rule again, choose Custom UDP Rule, set Port range to <code>1194</code>, and Protocol to UDP.</li> <li>Click Create security group.    </li> </ol> <p>Step 4: Launch the TryMOSK instance</p> <ol> <li>In the left navigation pane, choose Instances, then click Launch instances.</li> <li>Under Name and tags, set the Name (for example: <code>My TryMOSK</code>).</li> <li>Under Application and OS Images (Amazon Machine Image):</li> <li>Type <code>trymosk</code> in the search box.</li> <li>Select the required image from the results.</li> <li>Under Instance type, choose t2.xlarge (or t2.2xlarge for better performance).</li> <li>Under Key pair (login):</li> <li>Select an existing key pair, or</li> <li>Click Create new key pair to create one.</li> <li>Under Network settings, choose Select existing security group and select the one created in Step 3.</li> <li>Under Configure storage, set Root volume size to 40 GB or more.</li> <li>Click Launch instance.    </li> </ol> <p>Step 5: Get the instance\u2019s Public IP address</p> <ol> <li>Wait until the Instance state changes to Running.</li> <li>Select your instance and note the Public IPv4 address in the details panel.    </li> </ol> <p>Step 6: Connect to the instance via SSH</p> <ul> <li>Linux/macOS:</li> </ul> <pre><code>ssh -i /path/to/private-key.pem ubuntu@&lt;PublicIPv4Address&gt;\n</code></pre> <ul> <li>Windows (PuTTY):</li> <li>Convert your <code>.pem</code> key to <code>.ppk</code> using PuTTYgen.</li> <li>Open PuTTY, enter the public IPv4 address, and load your <code>.ppk</code> key in Connection \u2192 SSH \u2192 Auth.</li> </ul>"},{"location":"quick-start/trymosk-installation-aws/#creating-a-trymosk-instance-using-the-aws-cli","title":"Creating a TryMOSK Instance Using the AWS CLI","text":"<p>This procedure describes how to launch a TryMOSK EC2 instance using the AWS Command Line Interface (AWS CLI).</p> <p>Step 1: Install the AWS CLI</p> <p>Follow the installation instructions in the AWS CLI User Guide:  https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p> <p>Step 2: Configure the AWS CLI</p> <ol> <li>Follow the quick start configuration guide:     https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html</li> <li>Important: Set the default AWS region to us-east-2 to ensure all operations are performed in the correct region:</li> </ol> <pre><code>aws configure set region us-east-2\n</code></pre> <p>Step 3: Create or import a Key Pair</p> <p>You will use this key pair to connect to your EC2 instance via SSH.</p> <p>Option A \u2013 Create a new key pair</p> <pre><code>aws ec2 create-key-pair --key-type rsa --key-name &lt;your-key-name&gt; \\\n--query 'KeyMaterial' --output text &gt; &lt;private-key-file&gt;.pem\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;your-key-name&gt;</code> \u2013 unique key pair name (up to 255 ASCII characters).</li> <li><code>&lt;private-key-file&gt;</code> \u2013 file name for the private RSA key (keep it in a safe place).</li> </ul> <p>Example:</p> <pre><code>aws ec2 create-key-pair --key-type rsa --key-name my-key-for-trymosk \\\n--query 'KeyMaterial' --output text &gt; my_private_key.pem\n</code></pre> <p>Option B \u2013 Import an existing public key</p> <pre><code>aws ec2 import-key-pair --key-name &lt;your-key-name&gt; \\\n--public-key-material fileb://&lt;public-key-file&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;public-key-file&gt;</code> \u2013 path to the <code>.pub</code> public key file.</li> </ul> <p>Example:</p> <pre><code>aws ec2 import-key-pair --key-name another-key-for-trymosk \\\n--public-key-material fileb://home/ubuntu/test_rsa.pub --output text\n</code></pre> <p>Example response:</p> <pre><code>b8:d6:be:72:9c:60:d8:b8:59:d9:00:ab:bd:9d:8c:f8 another-key-for-trymosk key-00d4cf79ad02dd1c5\n</code></pre> <p>Step 4: Create a Security Group</p> <p>4.1. Find your default VPC ID <pre><code>aws ec2 describe-vpcs --query='Vpcs[?IsDefault].VpcId' --output text\n</code></pre> Example of response: <pre><code>  vpc-4744c62c\n</code></pre></p> <p>4.2. Create the security group <pre><code>aws ec2 create-security-group --group-name &lt;your sg name&gt; \\\n--description \"&lt;your sg description&gt;\" --vpc-id &lt;VPC Id&gt; \\\n--query='GroupId' --output text\n</code></pre> Where:</p> <ul> <li><code>&lt;sg-name&gt;</code> \u2013 security group name (unique within VPC, up to 255 characters, cannot start with <code>sg-</code>).</li> <li><code>&lt;sg-description&gt;</code> \u2013 description of the security group.</li> <li><code>&lt;VPC-ID&gt;</code> \u2013 ID from step 4.1</li> </ul> <p>Example: <pre><code>aws ec2 create-security-group --group-name trymosk-security-group \\\n--description \"Test security group for TryMOSK\" --vpc-id vpc-4744c62c \\\n--query='GroupId' --output text\n</code></pre> Example of response: <pre><code>sg-0d72aa991587b5648\n</code></pre></p> <p>4.3. Add inbound rules Allow SSH (port 22) and OpenVPN (port 1194 UDP): <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;sg-id&gt; --protocol tcp --port 22 --cidr 0.0.0.0/0\naws ec2 authorize-security-group-ingress --group-id &lt;sg-id&gt; --protocol udp --port 1194 --cidr 0.0.0.0/0\n</code></pre> Where <code>&lt;sg-id&gt;</code> is from step 4.2.</p> <p>Step 5: Find the TryMOSK AMI ID</p> <pre><code>aws ec2 describe-images --filters Name=name,Values=\"trymosk*\" \\\n--query='Images[*].[ImageId,Name,CreationDate]' --output table\n</code></pre> <p>Example response:</p> <pre><code>--------------------------------------------------------------------------------------------------\n|                                         DescribeImages                                         |\n+-----------------------+-------------------------------------------+----------------------------+\n|  ami-0504712c3ecb27331|  trymosk-jammy-amd64-25.1-20250723112122  |  2025-07-29T09:52:53.000Z  |\n+-----------------------+-------------------------------------------+----------------------------+\n</code></pre> <p>Step 6: Launch the TryMOSK instance</p> <pre><code>aws ec2 run-instances --image-id &lt;AMI Id&gt; --count 1 \\\n--instance-type t2.xlarge \\\n--key-name &lt;rsa key pair name&gt; \\\n--security-group-ids &lt;sg id&gt; \\\n--associate-public-ip-address \\\n--block-device-mapping DeviceName=/dev/sda1,Ebs={VolumeSize=&lt;root volume size&gt;} \\\n--query='Instances[0].InstanceId' --output text\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;AMI-ID&gt;</code> \u2013 AMI ID from step 5.</li> <li><code>&lt;rsa-key-name&gt;</code> \u2013 key pair from step 3.</li> <li><code>&lt;sg-id&gt;</code> \u2013 security group from step 4.</li> <li><code>&lt;root-size&gt;</code> \u2013 root volume size (minimum 40 GB).</li> </ul> <p>Example:</p> <pre><code>aws ec2 run-instances --image-id ami-0504712c3ecb27331 --count 1 \\\n--instance-type t2.xlarge \\\n--key-name another-key-for-trymosk \\\n--security-group-ids sg-0d72aa991587b5648 \\\n--associate-public-ip-address \\\n--block-device-mapping DeviceName=/dev/sda1,Ebs={VolumeSize=40} \\\n--query='Instances[0].InstanceId' --output text\n</code></pre> <p>Example of response:</p> <pre><code>i-0c8a0969dfd64c909\n</code></pre> <p>Step 7: Wait for the instance to start</p> <pre><code>aws ec2 describe-instances --instance-ids &lt;your instance Id&gt; \\\n--query='Reservations[0].Instances[0].State.Name' --output text\n</code></pre> <p>Where  <code>&lt;your instance Id&gt;</code> is from step 6. Example:</p> <pre><code>aws ec2 describe-instances --instance-ids i-0c8a0969dfd64c909 \\\n--query='Reservations[0].Instances[0].State.Name' --output text\n</code></pre> <p>Example of response:</p> <pre><code>pending\n</code></pre> <p>Step 8: Get the public IPv4 address</p> <pre><code>aws ec2 describe-instances --instance-ids &lt;your instance Id&gt; \\\n--query='Reservations[0].Instances[0].PublicIpAddress' --output text\n</code></pre> <p>Where <code>&lt;your instance Id&gt;</code> is from step 6. Example:</p> <pre><code>aws ec2 describe-instances --instance-ids i-0c8a0969dfd64c909 \\\n--query='Reservations[0].Instances[0].PublicIpAddress' --output text\n</code></pre> <p>Example of response:</p> <pre><code>18.218.29.107\n</code></pre> <p>Step 9: Connect via SSH</p> <p>From your local computer:</p> <ul> <li>Linux/macOS:</li> </ul> <pre><code>ssh -i /path/to/private-key.pem ubuntu@&lt;PublicIPv4Address&gt;\n</code></pre> <ul> <li>Windows (PuTTY):<ul> <li>Convert your <code>.pem</code> key to <code>.ppk</code> using PuTTYgen.</li> <li>Open PuTTY, enter the public IPv4 address, and load your <code>.ppk</code> key in Connection \u2192 SSH \u2192 Auth.</li> </ul> </li> </ul>"},{"location":"quick-start/trymosk-installation-aws/#setting-up-trymosk-on-an-aws-instance","title":"Setting up TryMOSK on an AWS instance","text":"<p>This section explains how to set up TryMOSK on the EC2 instance created in the previous steps.</p> <p>Step 1: Start the TryMOSK installation</p> <p>The installation process takes approximately 25 minutes. It will:</p> <ul> <li>Install and configure Kubernetes and Rockoon on the instance.</li> <li>Set up the OpenVPN server.</li> <li>Create the OpenVPN client configuration file.</li> </ul> <p>Run the following command to start the setup:</p> <pre><code>sudo screen -d -m /srv/trymosk/launch.sh\n</code></pre> <p>This runs the installation script inside a screen session, allowing it to continue even if your SSH connection is interrupted.</p> <p>Step 2: Monitor the installation</p> <p>You can view the installation log in real time:</p> <pre><code>sudo tail -f /var/log/trymosk-install.log\n</code></pre> <p>Step 3: Access TryMOSK after installation</p> <p>When installation is complete, you can:</p> <ol> <li>Work with MOS directly via the console, or</li> <li>Retrieve the OpenVPN client configuration file and set up a VPN connection from your local computer.</li> </ol> <p>Once connected via VPN, you can use:</p> <ul> <li>Horizon (OpenStack web interface)</li> <li>Mirantis Lens</li> <li>Local <code>kubectl</code> commands to manage TryMOSK services.</li> </ul> <p>Step 4: View system information and credentials</p> <p>After installation, system details, OpenStack version, and admin credentials will be displayed as a Message of the Day (MOTD) upon SSH login.</p> <p>Example output:</p> <pre><code>OS_FAMILY: Debian\nOS_DISTRO: Ubuntu\nOS_DISTRO_VERSION: 22.04\nOPENSTACK_VERSION: caracal\nROCKOON_VERSION: 1.0.11\nADMIN_USERNAME: adminlpsdb63Gvn1\nADMIN_PASSWORD: tLuQfuJVAxxxxxxxxxxxxx\n</code></pre>"},{"location":"quick-start/vpn-config/","title":"Configuring VPN","text":""},{"location":"quick-start/vpn-config/#ssh-tunnel-with-sshuttle","title":"SSH tunnel with sshuttle","text":"<ol> <li>Get IP address of ingress service. On your AIO instance run following command: <pre><code>kubectl -n openstack get svc ingress -o jsonpath='{.status.loadBalancer.ingress[].ip}'\n</code></pre></li> <li>Update local <code>/etc/hosts</code> file to point public domain to ingress external IP <pre><code>10.172.1.100 aodh.it.just.works barbican.it.just.works cinder.it.just.works cloudformation.it.just.works designate.it.just.works glance.it.just.works gnocchi.it.just.works heat.it.just.works horizon.it.just.works keystone.it.just.works metadata.it.just.works neutron.it.just.works nova.it.just.works novncproxy.it.just.works octavia.it.just.works placement.it.just.works spiceproxy.it.just.works\n</code></pre> where <code>10.172.1.100</code> is the result of previous command execution.</li> <li>Setup <code>sshuttle</code> to services external IPs <pre><code>sshuttle -r ubuntu@172.16.250.153 10.172.1.0/24\n</code></pre> where <code>172.16.250.153</code> is the public IP of your AIO instance</li> </ol>"},{"location":"quick-start/vpn-config/#openvpn-for-trymosk-installation","title":"OpenVPN (for TryMOSK installation)","text":"<p>By default, the TryMOSK installation process sets up an OpenVPN server on the instance and creates a client configuration file.</p> <ol> <li> <p>Copy OpenVPN client configuration file /src/vpn/client.ovpn to your local computer. Use <code>scp</code> or another secure file transfer method to download the file from the instance to your local machine.</p> </li> <li> <p>Update the server IP address in the configuration. In the <code>client.ovpn</code> file, replace the placeholder <code>&lt;Put your server public IP here&gt;</code> (found on line 4) with the actual public IPv4 address of your AWS instance. Example \u2013 before: <pre><code>...\nproto udp\nremote &lt;Put your server public IP here&gt; 1194\nresolv-retry infinite\n...\n</code></pre> Example \u2013 after: <pre><code>...\nproto udp\nremote 18.218.29.107 1194\nresolv-retry infinite\n...\n</code></pre> Where <code>18.218.29.107</code> is the public IPv4 address of your AWS EC2 instance.</p> </li> <li> <p>Use the configuration with an OpenVPN client such as:</p> <ul> <li>The official OpenVPN client: https://openvpn.net/client/</li> <li>Any compatible VPN client that supports OpenVPN connections</li> </ul> <p>Refer to your chosen VPN client\u2019s documentation for import and connection instructions.</p> </li> <li> <p>Configur DNS resolver. The OpenVPN server is configured to update the client\u2019s DNS resolver so that TryMOSK service URLs can be resolved correctly.</p> <ul> <li>macOS and Windows \u2013 OpenVPN clients apply this configuration automatically.</li> <li>Linux \u2013 Additional DNS configuration may be required, depending on your distribution, to ensure proper domain resolution for TryMOSK services. Consult your distribution\u2019s networking documentation for details. You also can update <code>/etc/hosts</code> file on your local computer as is described in step 2 for <code>sshuttle</code> setup.</li> </ul> </li> </ol>"},{"location":"user-guide/openstack/masakari/introspective-monitor/","title":"Introspective instance monitor configuration and test","text":"<p>This section describes how to test the <code>introspective instance monitor</code>.</p> <p>1. Enable the introspective instance monitor</p> <p>To deploy Masakari and enable the introspective instance monitor refer to Instance HA documentation</p> <p>2. Wait for the deployment to complete</p> <p>Wait for the <code>OpenStackDeploymentStatus</code> object reaches the APPLIED state and all applications are healthy. Please refer to OpenStackDeploymentStatus Custom Resource for more information.</p> <p>3. Create RSA keys for the test instance</p> <p>Generate an RSA key pair for the test instance you will create in the next step, and copy the public key to the <code>keystone-client</code> pod.</p> <pre><code>ssh-keygen -C '' -N '' -f test_key\nkeystone_pod=$(kubectl --namespace openstack get pod -l application=keystone,component=client --output jsonpath='{.items[0].metadata.name}')\nkubectl cp test_key.pub openstack/${keystone_pod}:/tmp/\n</code></pre> <p>4. Access the keystone-client pod</p> <p>Enter the <code>keystone-client</code> pod to perform OpenStack operations:</p> <pre><code>kubectl --namespace openstack exec -ti deployment/keystone-client -- bash\n</code></pre> <p>5. Configure OpenStack for Masakari service</p> <p>Inside the pod, run:</p> <p><pre><code>SEGMENT_UUID=$(openstack segment create allcomputes auto compute -f value -c uuid 2&gt;/dev/null)\nfor host in $(openstack hypervisor list -f value -c \"Hypervisor Hostname\" 2&gt;/dev/null); do \\\n    openstack segment host create ${host} compute SSH ${SEGMENT_UUID}; \\\ndone\nopenstack image set --property hw_qemu_guest_agent=yes Ubuntu-18.04\n</code></pre> For more information about the commands executed, please refer to the user guide</p> <p>6. Create a test instance</p> <p>Still inside the pod, execute:</p> <pre><code>openstack keypair create --public-key /tmp/test_key.pub test_key\nopenstack network create test_net\nopenstack subnet create test_subnet --network test_net --subnet-range 192.0.2.0/24 --allocation-pool start=192.0.2.2,end=192.0.2.20\nopenstack router create test_router\nopenstack router set test_router --external-gateway public\nopenstack router add subnet test_router test_subnet\nopenstack security group create test_sg\nopenstack security group rule create test_sg --remote-ip 0.0.0.0/0\nopenstack floating ip create --floating-ip-address 10.11.12.119 public\nopenstack server create --image Ubuntu-18.04 --flavor m1.small --key-name test_key --network test_net --security-group test_sg --property HA_Enabled=True test_server\nopenstack server add floating ip test_server 10.11.12.119\n</code></pre> <p>7. Connect to the test instance</p> <p>Exit the <code>keystone-client</code> pod and connect to the new instance using its floating IP and the RSA private key created in step 3:</p> <pre><code>ssh -l ubuntu -i test_key 10.11.12.119\n</code></pre> <p>8. Install and verify QEMU guest agent</p> <p>On the instance, run:</p> <pre><code>sudo apt update\nsudo apt install qemu-guest-agent\nsystemctl status qemu-guest-agent\n</code></pre> <p>9. Test Masakari functionality</p> <p>To simulate a system crash, stop the <code>qemu-guest-agent</code> service on the instance:</p> <pre><code>sudo systemctl stop qemu-guest-agent\n</code></pre> <p>Within approximately 40 seconds, Masakari should detect the issue and restart the instance.</p>"},{"location":"user-guide/openstack/masakari/masakari-configuration/","title":"Configuration Openstack with Masakari","text":"<p>This article describes how to configure Instance High Availability service with Openstack CLI. Before continuing please make sure that the Masakari and its components are enabled in the system as it described in the article</p>"},{"location":"user-guide/openstack/masakari/masakari-configuration/#group-compute-nodes-into-segments","title":"Group compute nodes into segments","text":"<p>The segment object is a logical grouping of compute nodes into zones also known as availability zones. The segment object enables the cloud operator to list, create, show details for, update, and delete segments.</p> <p>To create a segment named <code>allcomputes</code> with service_type = <code>compute</code>, and recovery_method = <code>auto</code>, run:</p> <pre><code>openstack segment create allcomputes auto compute\n</code></pre>"},{"location":"user-guide/openstack/masakari/masakari-configuration/#create-hosts-under-segments","title":"Create hosts under segments","text":"<p>The host object represents compute service hypervisors. A host belongs to a segment. The host can be any kind of virtual machine that has compute service running on it. The host object enables the operator to list, create, show details for, update, and delete hosts.</p> <p>To create a host under a given segment:</p> <ol> <li>Obtain the hypervisor hostname: <pre><code>openstack hypervisor list\n</code></pre></li> <li>Create the host under previously created segment. <pre><code>openstack segment host create \\\n    &lt;hypervisor hostname&gt; \\\n    compute \\\n    SSH \\\n    &lt;segment&gt;\n</code></pre> where:<ul> <li><code>&lt;hypervisor hostname&gt;</code> - hostname from the command's result on step 1.</li> <li><code>&lt;segment&gt;</code> - name or UUID of existing segment, created in previous section</li> </ul> </li> </ol> <p>For example: <pre><code>openstack segment host create \\\n    test-host-1 \\\n    compute \\\n    SSH \\\n    b8b0d7ca-1088-49db-a1e2-be004522f3d1\n</code></pre></p>"},{"location":"user-guide/openstack/masakari/masakari-configuration/#requirements-for-the-introspective-instance-monitor","title":"Requirements for the introspective instance monitor","text":"<p>For the introspective instance monitor to work correctly, the following conditions must be met:</p> <ol> <li>The images used to create the virtual machines have the <code>hw_qemu_guest_agent=yes</code> property set. To set it, you must run the command: <pre><code>openstack image set --property hw_qemu_guest_agent=yes Ubuntu-18.04\n</code></pre> where <code>Ubuntu-18.04</code> is the name of the image for which you want to set the property.</li> <li>Virtual machines must be created with the <code>HA_Enabled=True</code> property set <pre><code>openstack server create --flavor &lt;FLAVOR&gt; \\\n                        --image &lt;IMAGE&gt; \\\n                        --network &lt;NETWORK&gt; \\\n                        --property HA_Enabled=True \\\n                        &lt;SERVER_NAME&gt;\n</code></pre></li> <li>QEMU Guest Agent must be installed in the virtual machine after it is launched.</li> </ol>"}]}